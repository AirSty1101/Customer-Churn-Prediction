# Cost-Sensitive Learning Implementation Guide

## üìã Overview

Cost-Sensitive Learning ‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏ó‡∏Ñ‡∏ô‡∏¥‡∏Ñ‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏´‡∏ô‡∏î **cost (‡∏ï‡πâ‡∏ô‡∏ó‡∏∏‡∏ô)** ‡∏ó‡∏µ‡πà‡πÅ‡∏ï‡∏Å‡∏ï‡πà‡∏≤‡∏á‡∏Å‡∏±‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö errors ‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ model focus ‡∏ó‡∏µ‡πà‡∏Å‡∏≤‡∏£‡∏•‡∏î error ‡∏ó‡∏µ‡πà‡∏°‡∏µ cost ‡∏™‡∏π‡∏á

### ‡∏ó‡∏≥‡πÑ‡∏°‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ Cost-Sensitive Learning?

‡πÉ‡∏ô‡∏õ‡∏±‡∏ç‡∏´‡∏≤ Customer Churn Prediction:

- **False Negative (‡∏û‡∏•‡∏≤‡∏î Churn)** = cost ‡∏™‡∏π‡∏á‡∏°‡∏≤‡∏Å (‡∏™‡∏π‡∏ç‡πÄ‡∏™‡∏µ‡∏¢‡∏•‡∏π‡∏Å‡∏Ñ‡πâ‡∏≤ = ‡∏™‡∏π‡∏ç‡πÄ‡∏™‡∏µ‡∏¢‡∏£‡∏≤‡∏¢‡πÑ‡∏î‡πâ)
- **False Positive (‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢ Churn ‡∏ú‡∏¥‡∏î)** = cost ‡∏ï‡πà‡∏≥ (‡πÅ‡∏Ñ‡πà‡πÄ‡∏™‡∏µ‡∏¢‡∏ï‡πâ‡∏ô‡∏ó‡∏∏‡∏ô‡∏ï‡∏¥‡∏î‡∏ï‡πà‡∏≠‡∏•‡∏π‡∏Å‡∏Ñ‡πâ‡∏≤)

**‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á:**

- ‡∏û‡∏•‡∏≤‡∏î‡∏•‡∏π‡∏Å‡∏Ñ‡πâ‡∏≤ 1 ‡∏Ñ‡∏ô‡∏ó‡∏µ‡πà‡∏à‡∏∞ Churn = ‡∏™‡∏π‡∏ç‡πÄ‡∏™‡∏µ‡∏¢ 100,000 ‡∏ö‡∏≤‡∏ó
- ‡∏ï‡∏¥‡∏î‡∏ï‡πà‡∏≠‡∏•‡∏π‡∏Å‡∏Ñ‡πâ‡∏≤‡∏ú‡∏¥‡∏î 1 ‡∏Ñ‡∏ô = ‡πÄ‡∏™‡∏µ‡∏¢‡∏ï‡πâ‡∏ô‡∏ó‡∏∏‡∏ô 500 ‡∏ö‡∏≤‡∏ó
- **Cost Ratio = 100,000 / 500 = 200** (‡πÅ‡∏ï‡πà‡πÉ‡∏ô‡∏ó‡∏≤‡∏á‡∏õ‡∏è‡∏¥‡∏ö‡∏±‡∏ï‡∏¥‡πÉ‡∏ä‡πâ 5-20 ‡∏Å‡πá‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏û‡∏≠)

---

## üéØ Cost-Sensitive Learning vs Other Methods

| Method             | Approach                              | Pros                                                                | Cons                                 |
| ------------------ | ------------------------------------- | ------------------------------------------------------------------- | ------------------------------------ |
| **Class Weights**  | ‡∏õ‡∏£‡∏±‡∏ö‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å‡∏Ç‡∏≠‡∏á class ‡πÉ‡∏ô loss function | ‚úÖ ‡∏á‡πà‡∏≤‡∏¢<br>‚úÖ ‡πÑ‡∏°‡πà‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•                                      | ‚ö†Ô∏è ‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å‡∏Ñ‡∏á‡∏ó‡∏µ‡πà‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ó‡∏∏‡∏Å sample      |
| **SMOTE/ADASYN**   | ‡∏™‡∏£‡πâ‡∏≤‡∏á synthetic samples               | ‚úÖ Balance dataset                                                  | ‚ùå Overfitting<br>‚ùå ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏õ‡∏•‡∏≠‡∏° |
| **Cost-Sensitive** | ‡∏Å‡∏≥‡∏´‡∏ô‡∏î cost ‡∏ï‡πà‡∏≤‡∏á sample                | ‚úÖ Flexible<br>‚úÖ ‡∏™‡∏∞‡∏ó‡πâ‡∏≠‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏ó‡∏≤‡∏á‡∏ò‡∏∏‡∏£‡∏Å‡∏¥‡∏à<br>‚úÖ ‡πÑ‡∏°‡πà‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏õ‡∏•‡∏≠‡∏° | ‚ö†Ô∏è ‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≥‡∏´‡∏ô‡∏î cost ‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°         |

---

## üîß Implementation

### 1. Configuration (`config.py`)

```python
# === Cost-Sensitive Learning ===
USE_COST_SENSITIVE = True   # ‡πÄ‡∏õ‡∏¥‡∏î‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô cost-sensitive learning
COST_RATIO = 10.0           # ‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å‡∏Ç‡∏≠‡∏á minority class (Churn)
                            # ‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥: 5.0, 10.0, 15.0, 20.0
```

### 2. Create Sample Weights (`cost_sensitive.py`)

```python
from cost_sensitive import get_sample_weights

# ‡∏™‡∏£‡πâ‡∏≤‡∏á sample weights
sample_weights = get_sample_weights(
    y_train,
    method='cost_ratio',
    cost_ratio=10.0
)

# ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå:
# - Not Churn (0): weight = 1.0
# - Churn (1): weight = 10.0
```

### 3. Train with Sample Weights

```python
# XGBoost ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö sample_weight
xgb_model.fit(
    X_train, y_train,
    sample_weight=sample_weights
)
```

---

## üìä How It Works

### ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á: Cost Ratio = 10.0

**‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•:**

- Training samples: 1,000 ‡∏Ñ‡∏ô
  - Not Churn (0): 800 ‡∏Ñ‡∏ô ‚Üí weight = 1.0 each
  - Churn (1): 200 ‡∏Ñ‡∏ô ‚Üí weight = 10.0 each

**Total Weight:**

- Not Churn: 800 √ó 1.0 = 800
- Churn: 200 √ó 10.0 = 2,000
- **Total: 2,800**

**‡∏ú‡∏•‡∏Å‡∏£‡∏∞‡∏ó‡∏ö:**

- Model ‡∏à‡∏∞‡πÉ‡∏´‡πâ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏Å‡∏±‡∏ö Churn samples ‡∏°‡∏≤‡∏Å‡∏Å‡∏ß‡πà‡∏≤ Not Churn ‡∏ñ‡∏∂‡∏á **10 ‡πÄ‡∏ó‡πà‡∏≤**
- ‡∏ñ‡πâ‡∏≤ model ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢ Churn ‡∏ú‡∏¥‡∏î (False Negative) ‚Üí loss ‡∏™‡∏π‡∏á‡∏°‡∏≤‡∏Å
- Model ‡∏à‡∏∞‡∏û‡∏¢‡∏≤‡∏¢‡∏≤‡∏°**‡∏•‡∏î False Negative** ‡∏°‡∏≤‡∏Å‡∏Ç‡∏∂‡πâ‡∏ô ‚Üí **Recall ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ç‡∏∂‡πâ‡∏ô**

---

## üß™ Experiment: Finding Optimal Cost Ratio

### ‡∏ß‡∏¥‡∏ò‡∏µ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô

```bash
# ‡∏ó‡∏î‡∏™‡∏≠‡∏ö cost ratios ‡∏ï‡πà‡∏≤‡∏á‡πÜ
python experiment_cost_sensitive.py
```

‡∏™‡∏Ñ‡∏£‡∏¥‡∏õ‡∏ï‡πå‡∏ô‡∏µ‡πâ‡∏à‡∏∞‡∏ó‡∏î‡∏™‡∏≠‡∏ö cost ratios: **5.0, 10.0, 15.0, 20.0, 25.0**

### ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏ó‡∏µ‡πà‡∏Ñ‡∏≤‡∏î‡∏´‡∏ß‡∏±‡∏á

| Cost Ratio     | Accuracy | Precision | Recall | F1     | ROC-AUC |
| -------------- | -------- | --------- | ------ | ------ | ------- |
| 0.0 (Baseline) | 0.7880   | 0.4862    | 0.6895 | 0.5703 | 0.8379  |
| 5.0            | 0.7800   | 0.4700    | 0.7200 | 0.5700 | 0.8350  |
| 10.0           | 0.7700   | 0.4500    | 0.7500 | 0.5650 | 0.8300  |
| 15.0           | 0.7600   | 0.4300    | 0.7800 | 0.5600 | 0.8250  |
| 20.0           | 0.7500   | 0.4100    | 0.8000 | 0.5500 | 0.8200  |

**Pattern:**

- ‡∏¢‡∏¥‡πà‡∏á Cost Ratio ‡∏™‡∏π‡∏á ‚Üí **Recall ‡πÄ‡∏û‡∏¥‡πà‡∏°**, **Precision ‡∏•‡∏î**
- Trade-off ‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á Precision ‡πÅ‡∏•‡∏∞ Recall

---

## üéØ Choosing Optimal Cost Ratio

### ‡∏ß‡∏¥‡∏ò‡∏µ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å Cost Ratio

**1. Business-Driven Approach (‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥)**

```python
# ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏à‡∏≤‡∏Å cost ‡∏à‡∏£‡∏¥‡∏á
cost_false_negative = 100_000  # ‡∏™‡∏π‡∏ç‡πÄ‡∏™‡∏µ‡∏¢‡∏•‡∏π‡∏Å‡∏Ñ‡πâ‡∏≤
cost_false_positive = 500      # ‡∏ï‡πâ‡∏ô‡∏ó‡∏∏‡∏ô‡∏ï‡∏¥‡∏î‡∏ï‡πà‡∏≠‡∏•‡∏π‡∏Å‡∏Ñ‡πâ‡∏≤

cost_ratio = cost_false_negative / cost_false_positive
# = 100,000 / 500 = 200

# ‡πÅ‡∏ï‡πà‡πÉ‡∏ô‡∏ó‡∏≤‡∏á‡∏õ‡∏è‡∏¥‡∏ö‡∏±‡∏ï‡∏¥ ‡πÉ‡∏ä‡πâ 5-20 ‡∏Å‡πá‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏û‡∏≠
# ‡πÄ‡∏û‡∏£‡∏≤‡∏∞ cost_ratio ‡∏™‡∏π‡∏á‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ‡∏à‡∏∞‡∏ó‡∏≥‡πÉ‡∏´‡πâ Precision ‡∏ï‡πà‡∏≥‡∏°‡∏≤‡∏Å
```

**2. Metric-Driven Approach**

- ‡∏ñ‡πâ‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ **Recall ‚â• 70%** ‚Üí ‡∏•‡∏≠‡∏á cost_ratio = 10-15
- ‡∏ñ‡πâ‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ **F1 Score ‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î** ‚Üí ‡∏•‡∏≠‡∏á cost_ratio = 5-10
- ‡∏ñ‡πâ‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ **Balance Precision & Recall** ‚Üí ‡∏•‡∏≠‡∏á cost_ratio = 5-8

**3. Validation-Based Approach**

- ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏´‡∏•‡∏≤‡∏¢ cost ratios (5, 10, 15, 20)
- ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å cost ratio ‡∏ó‡∏µ‡πà‡πÉ‡∏´‡πâ **Validation Metrics** ‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î

---

## üìà Expected Results

### Comparison: Class Weights vs Cost-Sensitive

**Baseline (Class Weights - Run #2):**

- Accuracy: 0.7880
- Precision: 0.4862
- Recall: 0.6895
- F1: 0.5703
- ROC-AUC: 0.8379

**Cost-Sensitive (Cost Ratio = 10.0 - Expected):**

- Accuracy: 0.7700 (-1.8%)
- Precision: 0.4500 (-3.6%)
- Recall: **0.7500 (+6.0%)** ‚úÖ
- F1: 0.5650 (-0.5%)
- ROC-AUC: 0.8300 (-0.8%)

**Key Takeaway:**

- ‚úÖ **Recall ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ç‡∏∂‡πâ‡∏ô 6%** (‡∏à‡∏≤‡∏Å 68.95% ‚Üí 75%)
- ‚ö†Ô∏è Precision ‡∏•‡∏î‡∏•‡∏á‡πÄ‡∏•‡πá‡∏Å‡∏ô‡πâ‡∏≠‡∏¢ (‡∏à‡∏≤‡∏Å 48.62% ‚Üí 45%)
- ‚ö†Ô∏è ROC-AUC ‡∏•‡∏î‡∏•‡∏á‡πÄ‡∏•‡πá‡∏Å‡∏ô‡πâ‡∏≠‡∏¢ (‡∏à‡∏≤‡∏Å 0.8379 ‚Üí 0.83)

---

## üí° Recommendations

### ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÑ‡∏´‡∏£‡πà‡∏Ñ‡∏ß‡∏£‡πÉ‡∏ä‡πâ Cost-Sensitive Learning?

‚úÖ **‡∏Ñ‡∏ß‡∏£‡πÉ‡∏ä‡πâ‡πÄ‡∏°‡∏∑‡πà‡∏≠:**

1. **False Negative ‡∏°‡∏µ cost ‡∏™‡∏π‡∏á‡∏Å‡∏ß‡πà‡∏≤ False Positive ‡∏°‡∏≤‡∏Å**
   - ‡πÄ‡∏ä‡πà‡∏ô: Customer Churn, Fraud Detection, Medical Diagnosis
2. **‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÄ‡∏û‡∏¥‡πà‡∏° Recall** ‡πÇ‡∏î‡∏¢‡πÄ‡∏â‡∏û‡∏≤‡∏∞
3. **‡∏¢‡∏≠‡∏°‡∏£‡∏±‡∏ö Precision ‡∏•‡∏î‡∏•‡∏á‡πÑ‡∏î‡πâ**
4. **‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• cost ‡∏à‡∏£‡∏¥‡∏á‡∏à‡∏≤‡∏Å‡∏ò‡∏∏‡∏£‡∏Å‡∏¥‡∏à**

‚ùå **‡πÑ‡∏°‡πà‡∏Ñ‡∏ß‡∏£‡πÉ‡∏ä‡πâ‡πÄ‡∏°‡∏∑‡πà‡∏≠:**

1. **Precision ‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏°‡∏≤‡∏Å‡∏Å‡∏ß‡πà‡∏≤ Recall**
2. **‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ Balance ‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á Precision & Recall**
3. **‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• cost ‡∏ó‡∏µ‡πà‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô**

### Alternative: Threshold Tuning

‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡πÅ‡∏ô‡πà‡πÉ‡∏à‡∏ß‡πà‡∏≤ cost ratio ‡∏Ñ‡∏ß‡∏£‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏ó‡πà‡∏≤‡πÑ‡∏£ ‚Üí ‡∏•‡∏≠‡∏á **Threshold Tuning** ‡∏Å‡πà‡∏≠‡∏ô

```python
# ‡πÅ‡∏ó‡∏ô‡∏ó‡∏µ‡πà‡∏à‡∏∞‡πÉ‡∏ä‡πâ cost-sensitive
# ‡∏•‡∏≠‡∏á‡∏õ‡∏£‡∏±‡∏ö threshold ‡πÅ‡∏ó‡∏ô
y_pred = (y_pred_proba >= 0.4).astype(int)  # ‡∏•‡∏î threshold ‡∏à‡∏≤‡∏Å 0.5 ‚Üí 0.4
# ‚Üí Recall ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ç‡∏∂‡πâ‡∏ô, Precision ‡∏•‡∏î‡∏•‡∏á
```

**‡∏Ç‡πâ‡∏≠‡∏î‡∏µ:**

- ‚úÖ ‡∏á‡πà‡∏≤‡∏¢‡∏Å‡∏ß‡πà‡∏≤ (‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á retrain model)
- ‚úÖ ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡πÑ‡∏î‡πâ‡πÄ‡∏£‡πá‡∏ß
- ‚úÖ ‡∏õ‡∏£‡∏±‡∏ö‡πÑ‡∏î‡πâ‡∏ï‡∏•‡∏≠‡∏î‡πÄ‡∏ß‡∏•‡∏≤

---

## üöÄ Quick Start

### Run #6: Cost-Sensitive Learning

**1. Update config.py:**

```python
RUN_NUMBER = 6
RESAMPLING_METHOD = 'none'
USE_COST_SENSITIVE = True
COST_RATIO = 10.0
```

**2. Train model:**

```bash
python train_models.py
```

**3. (Optional) Experiment with different cost ratios:**

```bash
python experiment_cost_sensitive.py
```

---

## üìù Notes

### Limitations

1. **Cross-Validation ‡πÑ‡∏°‡πà‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö sample_weight**

   - `sklearn.cross_validate` ‡πÑ‡∏°‡πà‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö `sample_weight` ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö XGBoost
   - ‡∏ï‡πâ‡∏≠‡∏á skip CV ‡∏´‡∏£‡∏∑‡∏≠‡πÉ‡∏ä‡πâ manual CV

2. **Hyperparameter Tuning ‡∏¢‡∏≤‡∏Å‡∏Ç‡∏∂‡πâ‡∏ô**

   - ‡∏ï‡πâ‡∏≠‡∏á tune ‡∏ó‡∏±‡πâ‡∏á XGBoost parameters ‡πÅ‡∏•‡∏∞ cost_ratio
   - ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ã‡∏±‡∏ö‡∏ã‡πâ‡∏≠‡∏ô

3. **‡∏≠‡∏≤‡∏à Overfit ‡πÑ‡∏î‡πâ**
   - ‡∏ñ‡πâ‡∏≤ cost_ratio ‡∏™‡∏π‡∏á‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ model ‡∏≠‡∏≤‡∏à overfit ‡πÑ‡∏õ‡∏ó‡∏µ‡πà minority class

### Best Practices

1. **‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏à‡∏≤‡∏Å cost_ratio ‡∏ï‡πà‡∏≥‡πÜ** (5-10) ‡πÅ‡∏•‡πâ‡∏ß‡∏Ñ‡πà‡∏≠‡∏¢‡πÄ‡∏û‡∏¥‡πà‡∏°
2. **Monitor Validation Metrics** ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏î‡∏π overfitting
3. **‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏Å‡∏±‡∏ö Baseline** (Class Weights)
4. **‡πÉ‡∏ä‡πâ Business Metrics** (ROI, Cost Savings) ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡∏î‡∏™‡∏¥‡∏ô‡πÉ‡∏à

---

## üìö References

- [XGBoost Documentation - Sample Weight](https://xgboost.readthedocs.io/en/latest/python/python_api.html#xgboost.XGBClassifier.fit)
- [Cost-Sensitive Learning for Imbalanced Classification](https://machinelearningmastery.com/cost-sensitive-learning-for-imbalanced-classification/)
- [Scikit-learn - Sample Weight](https://scikit-learn.org/stable/modules/generated/sklearn.utils.class_weight.compute_sample_weight.html)
