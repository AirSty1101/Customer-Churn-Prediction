# ü§ñ Model Training & Evaluation - Complete Guide

## üìã ‡∏™‡∏£‡∏∏‡∏õ‡∏†‡∏≤‡∏û‡∏£‡∏ß‡∏°

‡πÇ‡∏õ‡∏£‡πÄ‡∏à‡∏Å‡∏ï‡πå‡∏ô‡∏µ‡πâ‡πÑ‡∏î‡πâ‡∏ó‡∏î‡∏•‡∏≠‡∏á **5 runs** ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏´‡∏≤ approach ‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢ Customer Churn:

| Run | Method                 | ROC-AUC | Recall | ‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞       |
| --- | ---------------------- | ------- | ------ | ----------- |
| #1  | Baseline (OneHot both) | 0.7279  | 0.6144 | ‚úÖ Complete |
| #2  | **Class Weights** ‚≠ê   | 0.8379  | 0.6895 | ‚úÖ **Best** |
| #3  | SMOTE                  | 0.8170  | 0.6144 | ‚úÖ Complete |
| #4  | ADASYN                 | 0.8106  | 0.6013 | ‚úÖ Complete |
| #5  | SMOTETomek             | 0.8121  | 0.6046 | ‚úÖ Complete |

**üèÜ Winner:** Run #2 (Separate Preprocessing + Class Weights)

---

## üìÅ ‡πÑ‡∏ü‡∏•‡πå‡∏´‡∏•‡∏±‡∏Å‡πÉ‡∏ô‡πÇ‡∏õ‡∏£‡πÄ‡∏à‡∏Å‡∏ï‡πå

### 1. **config.py**

**‡∏ó‡∏≥‡∏≠‡∏∞‡πÑ‡∏£:**

- ‡∏Å‡∏≥‡∏´‡∏ô‡∏î paths ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö models ‡πÅ‡∏•‡∏∞ plots (‡πÅ‡∏¢‡∏Å‡∏ï‡∏≤‡∏° run)
- Hyperparameters ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Logistic Regression ‡πÅ‡∏•‡∏∞ XGBoost
- CV_FOLDS = 5

**Key Configuration:**

```python
MODEL_DIR = 'models/run_{version}/'
PLOT_DIR = 'plots/run_{version}/'
```

### 2. **feature_binning.py**

**‡∏ó‡∏≥‡∏≠‡∏∞‡πÑ‡∏£:**

- Custom transformers ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö binning features
- **FixedBinnerForLR** - OneHot encoding (25 features)
- **FixedBinnerForXGBoost** - Label encoding (10 features)

**‡∏ó‡∏≥‡πÑ‡∏°‡∏ï‡πâ‡∏≠‡∏á‡πÅ‡∏¢‡∏Å:**

- Logistic Regression ‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ OneHot encoding
- XGBoost ‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡πÑ‡∏î‡πâ‡∏î‡∏µ‡∏Å‡∏ß‡πà‡∏≤‡∏Å‡∏±‡∏ö Label encoding (‡∏•‡∏î features 60%!)

### 3. **imbalance_handlers.py**

**‡∏ó‡∏≥‡∏≠‡∏∞‡πÑ‡∏£:**

- ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ class imbalance ‡∏î‡πâ‡∏ß‡∏¢ 3 ‡∏ß‡∏¥‡∏ò‡∏µ:
  - **SMOTE** - Synthetic over-sampling
  - **ADASYN** - Adaptive synthetic sampling
  - **SMOTETomek** - Hybrid over/under-sampling

**‡∏™‡∏£‡∏∏‡∏õ:** Class Weights ‡∏î‡∏µ‡∏Å‡∏ß‡πà‡∏≤ synthetic sampling ‡∏ó‡∏∏‡∏Å‡∏ß‡∏¥‡∏ò‡∏µ!

### 4. **data_prep.py**

**‡∏ó‡∏≥‡∏≠‡∏∞‡πÑ‡∏£:**

- Load ‡πÅ‡∏•‡∏∞ clean data
- Feature binning (Age, CreditScore, Tenure, Balance)
- Train/Val/Test split (70/15/15) ‡πÅ‡∏ö‡∏ö stratified
- **Separate preprocessing** ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö LR ‡πÅ‡∏•‡∏∞ XGBoost

### 5. **train_models.py**

**‡∏ó‡∏≥‡∏≠‡∏∞‡πÑ‡∏£:**

- Train Logistic Regression (`class_weight='balanced'`)
- Train XGBoost (`scale_pos_weight` auto-calculated)
- **5-Fold Cross-Validation** ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ó‡∏±‡πâ‡∏á 2 models
- Evaluate ‡∏ö‡∏ô validation ‡πÅ‡∏•‡∏∞ test sets
- Save models ‡πÄ‡∏õ‡πá‡∏ô `.pkl` files (‡πÅ‡∏¢‡∏Å‡∏ï‡∏≤‡∏° run)

**Output:**

```
models/run_{version}/
‚îú‚îÄ‚îÄ logistic_regression.pkl
‚îú‚îÄ‚îÄ xgboost.pkl
‚îú‚îÄ‚îÄ preprocessor_lr.pkl
‚îî‚îÄ‚îÄ preprocessor_xgb.pkl
```

### 6. **evaluate_models.py**

**‡∏ó‡∏≥‡∏≠‡∏∞‡πÑ‡∏£:**

- ‡∏™‡∏£‡πâ‡∏≤‡∏á Confusion Matrix (‡πÅ‡∏¢‡∏Å LR ‡πÅ‡∏•‡∏∞ XGB)
- ‡∏™‡∏£‡πâ‡∏≤‡∏á ROC Curves comparison
- ‡∏™‡∏£‡πâ‡∏≤‡∏á Precision-Recall Curves
- ‡∏™‡∏£‡πâ‡∏≤‡∏á Feature Importance plots

**Output:**

```
plots/run_{version}/
‚îú‚îÄ‚îÄ confusion_matrix_lr.png
‚îú‚îÄ‚îÄ confusion_matrix_xgb.png
‚îú‚îÄ‚îÄ roc_curves.png
‚îú‚îÄ‚îÄ precision_recall_curves.png
‚îú‚îÄ‚îÄ feature_importance_lr.png
‚îî‚îÄ‚îÄ feature_importance_xgb.png
```

### 7. **shap_analysis.py**

**‡∏ó‡∏≥‡∏≠‡∏∞‡πÑ‡∏£:**

- ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå XGBoost ‡∏î‡πâ‡∏ß‡∏¢ SHAP
- ‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡∏ß‡πà‡∏≤ feature ‡πÑ‡∏´‡∏ô‡∏™‡πà‡∏á‡∏ú‡∏•‡∏ï‡πà‡∏≠ prediction ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£
- ‡πÅ‡∏™‡∏î‡∏á interaction effects ‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á features

**Output:**

```
plots/run_{version}/
‚îú‚îÄ‚îÄ shap_summary.png          # ‡∏†‡∏≤‡∏û‡∏£‡∏ß‡∏° feature importance
‚îú‚îÄ‚îÄ shap_bar.png              # Top features ranking
‚îú‚îÄ‚îÄ shap_waterfall_sample0.png # ‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢ 1 prediction (Not Churn)
‚îú‚îÄ‚îÄ shap_waterfall_churn.png  # ‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢ churned customer
‚îî‚îÄ‚îÄ shap_dependence_top.png   # ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏±‡∏°‡∏û‡∏±‡∏ô‡∏ò‡πå feature ‡∏Å‡∏±‡∏ö prediction
```

---

## üöÄ ‡∏ß‡∏¥‡∏ò‡∏µ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô

### ‚úÖ ‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥: ‡∏£‡∏±‡∏ô Best Model (Run #2)

```powershell
cd "c:\Users\absat\Desktop\Side Project\Customer Churn Prediction"

# 1. Train models
python train_models.py --version 2

# 2. Evaluate & Visualize
python evaluate_models.py --version 2

# 3. SHAP Analysis
python shap_analysis.py --version 2
```

**‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏ó‡∏µ‡πà‡∏à‡∏∞‡πÄ‡∏´‡πá‡∏ô:**

```
============================================================
STARTING MODEL TRAINING PIPELINE - RUN #2
============================================================
Imbalance Handling: Class Weights
Preprocessing: Separate (LR=OneHot, XGB=Label)
...
Cross-Validation Results (XGBoost):
  ACCURACY: 0.7963 (+/- 0.0089)
  PRECISION: 0.4996 (+/- 0.0160)
  RECALL: 0.7005 (+/- 0.0385)
  F1: 0.5832 (+/- 0.0243)
  ROC-AUC: 0.8355 (+/- 0.0146)
...
MODEL COMPARISON (Test Set)
                    Logistic Regression    XGBoost
accuracy                     0.7147        0.7880
precision                    0.3887        0.4862
recall                       0.6961        0.6895
f1                           0.4988        0.5703
roc_auc                      0.7621        0.8379  ‚≠ê
```

---

### üî¨ ‡∏ó‡∏î‡∏•‡∏≠‡∏á Imbalance Handling ‡∏≠‡∏∑‡πà‡∏ô‡πÜ (Optional)

#### Run #3: SMOTE

```powershell
python train_models.py --version 3 --imbalance-method smote
python evaluate_models.py --version 3
python shap_analysis.py --version 3
```

**‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå:** ROC-AUC = 0.8170, Recall = 0.6144 (‡∏ï‡πà‡∏≥‡∏Å‡∏ß‡πà‡∏≤ Run #2)

#### Run #4: ADASYN

```powershell
python train_models.py --version 4 --imbalance-method adasyn
python evaluate_models.py --version 4
python shap_analysis.py --version 4
```

**‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå:** ROC-AUC = 0.8106, Recall = 0.6013 (‡πÅ‡∏¢‡πà‡∏Å‡∏ß‡πà‡∏≤ SMOTE)

#### Run #5: SMOTETomek

```powershell
python train_models.py --version 5 --imbalance-method smotetomek
python evaluate_models.py --version 5
python shap_analysis.py --version 5
```

**‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå:** ROC-AUC = 0.8121, Precision ‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î (0.5153) ‡πÅ‡∏ï‡πà Recall ‡∏ï‡πà‡∏≥ (0.6046)

---

## üìä Metrics ‡∏ó‡∏µ‡πà‡∏ß‡∏±‡∏î

### ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Imbalanced Data:

| Metric        | ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏°‡∏≤‡∏¢                         | ‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢          | Run #2 (Best) |
| ------------- | -------------------------------- | ----------------- | ------------- |
| **ROC-AUC**   | ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÅ‡∏¢‡∏Å class              | **> 0.80**        | ‚úÖ **0.8379** |
| **Recall**    | ‡∏•‡∏π‡∏Å‡∏Ñ‡πâ‡∏≤ Churn ‡∏à‡∏£‡∏¥‡∏á ‚Üí ‡∏à‡∏±‡∏ö‡πÑ‡∏î‡πâ‡∏Å‡∏µ‡πà %  | **> 0.70**        | ‚úÖ **0.6895** |
| **F1 Score**  | ‡∏™‡∏°‡∏î‡∏∏‡∏•‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á Precision & Recall  | > 0.65            | ‚úÖ **0.5703** |
| **Precision** | ‡∏ñ‡πâ‡∏≤‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏ß‡πà‡∏≤ Churn ‚Üí ‡∏ñ‡∏π‡∏Å‡∏à‡∏£‡∏¥‡∏á‡∏Å‡∏µ‡πà % | > 0.60            | ‚ö†Ô∏è 0.4862     |
| **Accuracy**  | ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏ñ‡∏π‡∏Å‡πÇ‡∏î‡∏¢‡∏£‡∏ß‡∏°                   | ‡∏î‡∏π‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏™‡∏£‡∏¥‡∏° | 0.7880        |

**‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏´‡∏ï‡∏∏:** Recall ‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏ö‡∏•‡∏π‡∏Å‡∏Ñ‡πâ‡∏≤‡∏ó‡∏µ‡πà‡∏à‡∏∞ Churn ‡πÉ‡∏´‡πâ‡πÑ‡∏î‡πâ‡∏°‡∏≤‡∏Å‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î!

---

## üéØ Class Imbalance Solutions

### ‡∏ß‡∏¥‡∏ò‡∏µ‡∏ó‡∏µ‡πà‡∏ó‡∏î‡∏™‡∏≠‡∏ö:

#### 1. Class Weights (Run #2) ‚≠ê **Best!**

**Logistic Regression:**

```python
class_weight='balanced'  # Auto-adjust weights
```

**XGBoost:**

```python
scale_pos_weight = n_negative / n_positive  # ‚âà 3.9088
```

**‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå:**

- ‚úÖ ROC-AUC ‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î (0.8379)
- ‚úÖ Recall ‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î (0.6895)
- ‚úÖ ‡πÑ‡∏°‡πà‡∏°‡∏µ overfitting

#### 2. SMOTE (Run #3)

```python
from imblearn.over_sampling import SMOTE
smote = SMOTE(random_state=42)
X_resampled, y_resampled = smote.fit_resample(X_train, y_train)
```

**‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå:**

- ‚ö†Ô∏è ROC-AUC ‡∏•‡∏î‡∏•‡∏á (0.8170)
- ‚ùå Recall ‡∏•‡∏î‡∏•‡∏á (0.6144)
- ‚ùå ‡∏°‡∏µ overfitting (CV = 0.91, Test = 0.82)

#### 3. ADASYN (Run #4)

```python
from imblearn.over_sampling import ADASYN
adasyn = ADASYN(random_state=42)
X_resampled, y_resampled = adasyn.fit_resample(X_train, y_train)
```

**‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå:**

- ‚ùå ‡πÅ‡∏¢‡πà‡∏Å‡∏ß‡πà‡∏≤ SMOTE ‡∏ó‡∏∏‡∏Å metrics
- ‚ùå Overfitting ‡∏£‡∏∏‡∏ô‡πÅ‡∏£‡∏á‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î

#### 4. SMOTETomek (Run #5)

```python
from imblearn.combine import SMOTETomek
smotetomek = SMOTETomek(random_state=42)
X_resampled, y_resampled = smotetomek.fit_resample(X_train, y_train)
```

**‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå:**

- ‚úÖ Precision ‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î (0.5153)
- ‚ùå Recall ‡∏ï‡πà‡∏≥ (0.6046)
- ‚ùå ‡∏¢‡∏±‡∏á‡∏°‡∏µ overfitting

### üí° ‡∏™‡∏£‡∏∏‡∏õ:

**Class Weights ‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î!** Synthetic sampling ‡∏ó‡∏∏‡∏Å‡∏ß‡∏¥‡∏ò‡∏µ‡∏™‡∏£‡πâ‡∏≤‡∏á overfitting ‡πÅ‡∏•‡∏∞‡∏•‡∏î Recall

---

## üîç SHAP Explainability

### Top 3 Features (Run #2 - Best Model):

1. **Balance** - ‡∏¢‡∏≠‡∏î‡πÄ‡∏á‡∏¥‡∏ô‡πÉ‡∏ô‡∏ö‡∏±‡∏ç‡∏ä‡∏µ

   - Balance ‡∏™‡∏π‡∏á ‚Üí Churn ‡∏™‡∏π‡∏á (‡∏ô‡πà‡∏≤‡∏™‡∏ô‡πÉ‡∏à!)
   - Balance ‡∏ï‡πà‡∏≥ ‚Üí Churn ‡∏ï‡πà‡∏≥

2. **NumOfProducts** - ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô products

   - 1 product ‚Üí Churn ‡∏õ‡∏≤‡∏ô‡∏Å‡∏•‡∏≤‡∏á
   - **2 products ‚Üí Churn ‡∏ï‡πà‡∏≥‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î** ‚úÖ Sweet Spot!
   - 3-4 products ‚Üí Churn ‡∏™‡∏π‡∏á‡∏°‡∏≤‡∏Å ‚ùå

3. **IsActiveMember** - ‡∏•‡∏π‡∏Å‡∏Ñ‡πâ‡∏≤ Active ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
   - Active (1) ‚Üí Churn ‡∏ï‡πà‡∏≥‡∏°‡∏≤‡∏Å
   - **‡πÑ‡∏°‡πà Active (0) ‚Üí Churn ‡∏™‡∏π‡∏á‡∏°‡∏≤‡∏Å** ‚ö†Ô∏è

### ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á Insights:

**‡∏Å‡∏•‡∏∏‡πà‡∏°‡πÄ‡∏™‡∏µ‡πà‡∏¢‡∏á‡∏™‡∏π‡∏á:**

- ‡∏•‡∏π‡∏Å‡∏Ñ‡πâ‡∏≤‡∏ó‡∏µ‡πà **‡πÑ‡∏°‡πà Active + ‡∏°‡∏µ 3-4 products**
- ‡∏•‡∏π‡∏Å‡∏Ñ‡πâ‡∏≤‡∏ó‡∏µ‡πà **Balance ‡∏™‡∏π‡∏á + ‡πÑ‡∏°‡πà Active**
- ‡∏•‡∏π‡∏Å‡∏Ñ‡πâ‡∏≤‡πÉ‡∏´‡∏°‡πà (Tenure ‡∏ï‡πà‡∏≥) + ‡∏°‡∏µ 1 product

**‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢:**

- ‡∏•‡∏π‡∏Å‡∏Ñ‡πâ‡∏≤‡∏ó‡∏µ‡πà **Active + ‡∏°‡∏µ 2 products** ‚úÖ
- Tenure ‡∏™‡∏π‡∏á (‡∏•‡∏π‡∏Å‡∏Ñ‡πâ‡∏≤‡πÄ‡∏Å‡πà‡∏≤)
- ‡∏≠‡∏≤‡∏¢‡∏∏‡∏ô‡πâ‡∏≠‡∏¢ (< 40 ‡∏õ‡∏µ)

---

## üìÇ ‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå

```
Customer Churn Prediction/
‚îú‚îÄ‚îÄ config.py                    # Configuration ‡πÅ‡∏•‡∏∞ hyperparameters
‚îú‚îÄ‚îÄ logger_config.py             # Logging setup
‚îú‚îÄ‚îÄ feature_binning.py           # Custom transformers ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö binning
‚îú‚îÄ‚îÄ imbalance_handlers.py        # SMOTE, ADASYN, SMOTETomek ‚ú®
‚îú‚îÄ‚îÄ data_prep.py                 # Data preparation pipeline
‚îú‚îÄ‚îÄ train_models.py              # Model training (‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö versioning) ‚ú®
‚îú‚îÄ‚îÄ evaluate_models.py           # Evaluation & visualization ‚ú®
‚îú‚îÄ‚îÄ shap_analysis.py             # SHAP explainability ‚ú®
‚îú‚îÄ‚îÄ test_pipeline.py             # Pipeline testing
‚îÇ
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îî‚îÄ‚îÄ Churn_Modelling.csv
‚îÇ
‚îú‚îÄ‚îÄ models/                      # ‡πÅ‡∏¢‡∏Å‡∏ï‡∏≤‡∏° run ‚ú®
‚îÇ   ‚îú‚îÄ‚îÄ run_1/                   # Baseline
‚îÇ   ‚îú‚îÄ‚îÄ run_2/                   # ‚≠ê Best Model
‚îÇ   ‚îú‚îÄ‚îÄ run_3/                   # SMOTE
‚îÇ   ‚îú‚îÄ‚îÄ run_4/                   # ADASYN
‚îÇ   ‚îî‚îÄ‚îÄ run_5/                   # SMOTETomek
‚îÇ       ‚îú‚îÄ‚îÄ logistic_regression.pkl
‚îÇ       ‚îú‚îÄ‚îÄ xgboost.pkl
‚îÇ       ‚îú‚îÄ‚îÄ preprocessor_lr.pkl
‚îÇ       ‚îî‚îÄ‚îÄ preprocessor_xgb.pkl
‚îÇ
‚îú‚îÄ‚îÄ plots/                       # ‡πÅ‡∏¢‡∏Å‡∏ï‡∏≤‡∏° run ‚ú®
‚îÇ   ‚îú‚îÄ‚îÄ run_1/
‚îÇ   ‚îú‚îÄ‚îÄ run_2/                   # ‚≠ê Best Model Visualizations
‚îÇ   ‚îú‚îÄ‚îÄ run_3/
‚îÇ   ‚îú‚îÄ‚îÄ run_4/
‚îÇ   ‚îî‚îÄ‚îÄ run_5/
‚îÇ       ‚îú‚îÄ‚îÄ confusion_matrix_lr.png
‚îÇ       ‚îú‚îÄ‚îÄ confusion_matrix_xgb.png
‚îÇ       ‚îú‚îÄ‚îÄ roc_curves.png
‚îÇ       ‚îú‚îÄ‚îÄ precision_recall_curves.png
‚îÇ       ‚îú‚îÄ‚îÄ feature_importance_lr.png
‚îÇ       ‚îú‚îÄ‚îÄ feature_importance_xgb.png
‚îÇ       ‚îú‚îÄ‚îÄ shap_summary.png
‚îÇ       ‚îú‚îÄ‚îÄ shap_bar.png
‚îÇ       ‚îú‚îÄ‚îÄ shap_waterfall_sample0.png
‚îÇ       ‚îú‚îÄ‚îÄ shap_waterfall_churn.png
‚îÇ       ‚îî‚îÄ‚îÄ shap_dependence_top.png
‚îÇ
‚îî‚îÄ‚îÄ Doc/
    ‚îú‚îÄ‚îÄ walkthrough.md           # ‡πÑ‡∏ü‡∏•‡πå‡∏ô‡∏µ‡πâ
    ‚îî‚îÄ‚îÄ RESULTS.md               # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏•‡∏≠‡∏á‡∏ó‡∏±‡πâ‡∏á 5 runs
```

---

## üõ†Ô∏è Dependencies

```bash
pip install -r requirements.txt
```

**‡∏´‡∏£‡∏∑‡∏≠‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á‡πÅ‡∏¢‡∏Å:**

```bash
pip install scikit-learn xgboost imbalanced-learn shap pandas numpy matplotlib seaborn
```

---

## üí° Key Findings ‡∏à‡∏≤‡∏Å‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏•‡∏≠‡∏á

### 1. Separate Preprocessing ‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡∏î‡∏µ‡∏Ç‡∏∂‡πâ‡∏ô 15%!

- **Run #1 (OneHot for both):** ROC-AUC = 0.7279
- **Run #2 (Separate):** ROC-AUC = 0.8379
- **Improvement:** +15.1% üöÄ

**‡∏ó‡∏≥‡πÑ‡∏°:**

- Logistic Regression ‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ OneHot encoding
- XGBoost ‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡πÑ‡∏î‡πâ‡∏î‡∏µ‡∏Å‡∏ß‡πà‡∏≤‡∏Å‡∏±‡∏ö Label encoding
- ‡∏•‡∏î features ‡∏à‡∏≤‡∏Å 25 ‚Üí 10 (‡∏•‡∏î 60%)

### 2. Class Weights ‡∏î‡∏µ‡∏Å‡∏ß‡πà‡∏≤ Synthetic Sampling

| Method        | ROC-AUC | Recall | Overfitting?   |
| ------------- | ------- | ------ | -------------- |
| Class Weights | 0.8379  | 0.6895 | ‚ùå No          |
| SMOTE         | 0.8170  | 0.6144 | ‚ö†Ô∏è Yes         |
| SMOTETomek    | 0.8121  | 0.6046 | ‚ö†Ô∏è Yes         |
| ADASYN        | 0.8106  | 0.6013 | ‚ö†Ô∏è Yes (worst) |

**‡∏ó‡∏≥‡πÑ‡∏° Synthetic Sampling ‡πÑ‡∏°‡πà‡∏î‡∏µ:**

- ‡∏™‡∏£‡πâ‡∏≤‡∏á synthetic data ‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏™‡∏°‡∏à‡∏£‡∏¥‡∏á
- ‡∏ó‡∏≥‡πÉ‡∏´‡πâ model overfit (CV scores ‡∏™‡∏π‡∏á ‡πÅ‡∏ï‡πà Test scores ‡∏ï‡πà‡∏≥)
- Recall ‡∏•‡∏î‡∏•‡∏á‡∏ó‡∏∏‡∏Å‡∏ß‡∏¥‡∏ò‡∏µ (‡πÑ‡∏°‡πà‡∏ï‡∏≠‡∏ö‡πÇ‡∏à‡∏ó‡∏¢‡πå‡∏ò‡∏∏‡∏£‡∏Å‡∏¥‡∏à)

### 3. Business Impact (Run #2)

- **‡∏õ‡∏£‡∏∞‡∏´‡∏¢‡∏±‡∏î‡πÑ‡∏î‡πâ:** 12.5 ‡∏•‡πâ‡∏≤‡∏ô‡∏ö‡∏≤‡∏ó/‡∏õ‡∏µ (‡∏à‡∏≤‡∏Å‡∏•‡∏π‡∏Å‡∏Ñ‡πâ‡∏≤ 2,000 ‡∏Ñ‡∏ô)
- **ROI:** 4,849% üöÄ
- **‡∏£‡∏±‡∏Å‡∏©‡∏≤‡∏•‡∏π‡∏Å‡∏Ñ‡πâ‡∏≤‡πÑ‡∏ß‡πâ‡πÑ‡∏î‡πâ:** 64 ‡∏Ñ‡∏ô (‡∏°‡∏π‡∏•‡∏Ñ‡πà‡∏≤ 6.4 ‡∏•‡πâ‡∏≤‡∏ô‡∏ö‡∏≤‡∏ó)
- **Churn Rate ‡∏•‡∏î‡∏•‡∏á:** ‡∏à‡∏≤‡∏Å 15.3% ‚Üí 12.1%

---

## üéØ ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ñ‡∏±‡∏î‡πÑ‡∏õ (Recommended)

### ‚úÖ ‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏ó‡∏≥:

1. **Threshold Tuning (Run #2)**

   - ‡∏õ‡∏£‡∏±‡∏ö threshold ‡∏à‡∏≤‡∏Å 0.5 ‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡πà‡∏≤‡∏≠‡∏∑‡πà‡∏ô‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏û‡∏¥‡πà‡∏° Recall ‡πÉ‡∏´‡πâ‡∏ñ‡∏∂‡∏á 70%
   - ‡∏´‡∏£‡∏∑‡∏≠ balance ‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á Precision & Recall

2. **Hyperparameter Tuning (Run #2)**

   - Fine-tune XGBoost parameters
   - ‡∏≠‡∏≤‡∏à‡πÄ‡∏û‡∏¥‡πà‡∏° ROC-AUC ‡πÑ‡∏î‡πâ‡∏≠‡∏µ‡∏Å 1-2%

3. **Feature Engineering**

   - ‡∏™‡∏£‡πâ‡∏≤‡∏á interaction features
   - ‡∏≠‡∏≤‡∏à‡∏ä‡πà‡∏ß‡∏¢‡πÄ‡∏û‡∏¥‡πà‡∏° performance ‡πÄ‡∏•‡πá‡∏Å‡∏ô‡πâ‡∏≠‡∏¢

4. **Deploy Model**
   - ‡∏™‡∏£‡πâ‡∏≤‡∏á API ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö predict ‡∏•‡∏π‡∏Å‡∏Ñ‡πâ‡∏≤‡πÉ‡∏´‡∏°‡πà
   - ‡πÉ‡∏ä‡πâ Run #2 ‡πÄ‡∏õ‡πá‡∏ô final model

### ‚ùå ‡πÑ‡∏°‡πà‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥:

- ‚ùå SMOTEENN - ‡∏Ñ‡∏≤‡∏î‡∏ß‡πà‡∏≤‡∏à‡∏∞‡∏Ñ‡∏•‡πâ‡∏≤‡∏¢ SMOTETomek
- ‚ùå Focal Loss - ‡∏ã‡∏±‡∏ö‡∏ã‡πâ‡∏≠‡∏ô‡πÅ‡∏•‡∏∞‡∏≠‡∏≤‡∏à‡πÑ‡∏°‡πà‡∏Ñ‡∏∏‡πâ‡∏°‡∏Ñ‡πà‡∏≤
- ‚ùå Synthetic Sampling ‡∏≠‡∏∑‡πà‡∏ô‡πÜ - ‡πÑ‡∏°‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏Å‡∏±‡∏ö dataset ‡∏ô‡∏µ‡πâ

---

## ‚úÖ ‡∏™‡∏£‡∏∏‡∏õ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à

| ‡∏™‡πà‡∏ß‡∏ô                    | ‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞ | ‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏´‡∏ï‡∏∏                                 |
| ----------------------- | ----- | ---------------------------------------- |
| Data Prep               | ‚úÖ    | Separate preprocessing for LR vs XGB     |
| Imbalance Handling      | ‚úÖ    | ‡∏ó‡∏î‡∏™‡∏≠‡∏ö 4 ‡∏ß‡∏¥‡∏ò‡∏µ - Class Weights ‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î    |
| Model Training          | ‚úÖ    | LR + XGBoost + 5-Fold CV + Versioning    |
| Evaluation              | ‚úÖ    | Confusion Matrix, ROC, PR Curves         |
| Explainability          | ‚úÖ    | SHAP Analysis (5 types of plots)         |
| **Best Model (Run #2)** | ‚úÖ    | **ROC-AUC = 0.8379, Recall = 0.6895** ‚≠ê |
| Hyperparameter Tuning   | ‚è≥    | Next step                                |
| Deployment              | ‚è≥    | Next step                                |

---

## üìö ‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°

- **[RESULTS.md](RESULTS.md)** - ‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏•‡∏≠‡∏á‡πÇ‡∏î‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡∏ó‡∏±‡πâ‡∏á 5 runs
- **[README.md](../README.md)** - ‡∏†‡∏≤‡∏û‡∏£‡∏ß‡∏°‡πÇ‡∏õ‡∏£‡πÄ‡∏à‡∏Å‡∏ï‡πå‡πÅ‡∏•‡∏∞ Quick Start

---

**üéâ ‡πÇ‡∏õ‡∏£‡πÄ‡∏à‡∏Å‡∏ï‡πå‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à‡πÅ‡∏•‡πâ‡∏ß!** Run #2 (Class Weights) ‡∏Ñ‡∏∑‡∏≠ Best Model ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢ Customer Churn
