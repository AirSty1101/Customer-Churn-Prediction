# Customer Churn Prediction

‡∏£‡∏∞‡∏ö‡∏ö‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏´‡∏¢‡∏∏‡∏î‡πÉ‡∏ä‡πâ‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£‡∏Ç‡∏≠‡∏á‡∏•‡∏π‡∏Å‡∏Ñ‡πâ‡∏≤‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£ (Customer Churn) ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ Machine Learning

## üìä ‡∏†‡∏≤‡∏û‡∏£‡∏ß‡∏°‡πÇ‡∏õ‡∏£‡πÄ‡∏à‡∏Å‡∏ï‡πå

‡πÇ‡∏õ‡∏£‡πÄ‡∏à‡∏Å‡∏ï‡πå‡∏ô‡∏µ‡πâ‡πÉ‡∏ä‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏•‡∏π‡∏Å‡∏Ñ‡πâ‡∏≤‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏ß‡πà‡∏≤‡∏•‡∏π‡∏Å‡∏Ñ‡πâ‡∏≤‡∏Ñ‡∏ô‡πÑ‡∏´‡∏ô‡∏°‡∏µ‡πÅ‡∏ô‡∏ß‡πÇ‡∏ô‡πâ‡∏°‡∏à‡∏∞‡∏´‡∏¢‡∏∏‡∏î‡πÉ‡∏ä‡πâ‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£ (Churn) ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ:

- **Logistic Regression** (Baseline model)
- **XGBoost** (High-performance model)
- **Hyperparameter Tuning** (Optimized model)
- **Threshold Tuning** (Balanced predictions)
- **SHAP** ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢

## üöÄ Quick Start

### 1. ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á Dependencies

```bash
pip install -r requirements.txt
```

### 2. ‡∏£‡∏±‡∏ô Best Model (Run #2.2 - Hyperparameter Tuned + Threshold 0.54)

```bash
# Train models with optimized hyperparameters
python train_models.py

# ‡∏™‡∏£‡πâ‡∏≤‡∏á visualizations
python evaluate_models.py

# ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏î‡πâ‡∏ß‡∏¢ SHAP
python shap_analysis.py
```

### 3. ‡∏ó‡∏î‡∏•‡∏≠‡∏á Approaches ‡∏≠‡∏∑‡πà‡∏ô‡πÜ (Optional)

```bash
# Hyperparameter Tuning
python hyperparameter_tuning.py

# Threshold Tuning
python threshold_tuning.py

# Cost-Sensitive Learning
python train_models.py --cost-sensitive
```

**‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏´‡∏ï‡∏∏:** Run #2.2 (Hyperparameter Tuned + Threshold 0.54) ‡πÉ‡∏´‡πâ‡∏ú‡∏•‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î!

## üìÅ ‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏õ‡∏£‡πÄ‡∏à‡∏Å‡∏ï‡πå

```
Customer Churn Prediction/
‚îú‚îÄ‚îÄ README.md                    # ‡πÑ‡∏ü‡∏•‡πå‡∏ô‡∏µ‡πâ
‚îú‚îÄ‚îÄ requirements.txt             # Python dependencies
‚îú‚îÄ‚îÄ config.py                    # Configuration ‡πÅ‡∏•‡∏∞ hyperparameters
‚îú‚îÄ‚îÄ logger_config.py             # Logging setup
‚îÇ
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îî‚îÄ‚îÄ Churn_Modelling.csv     # Dataset (10,000 ‡∏•‡∏π‡∏Å‡∏Ñ‡πâ‡∏≤)
‚îÇ
‚îú‚îÄ‚îÄ feature_binning.py           # Custom transformers ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö binning
‚îú‚îÄ‚îÄ imbalance_handlers.py        # SMOTE, ADASYN, SMOTETomek handlers
‚îú‚îÄ‚îÄ cost_sensitive.py            # Cost-sensitive learning utilities
‚îú‚îÄ‚îÄ data_prep.py                 # Data preparation pipeline
‚îú‚îÄ‚îÄ train_models.py              # Model training script
‚îú‚îÄ‚îÄ evaluate_models.py           # Evaluation & visualization
‚îú‚îÄ‚îÄ shap_analysis.py             # SHAP explainability
‚îú‚îÄ‚îÄ hyperparameter_tuning.py     # Hyperparameter optimization
‚îú‚îÄ‚îÄ threshold_tuning.py          # Threshold optimization
‚îÇ
‚îú‚îÄ‚îÄ models/                      # Trained models (‡πÅ‡∏¢‡∏Å‡∏ï‡∏≤‡∏° run)
‚îÇ   ‚îú‚îÄ‚îÄ run_1/                   # Baseline (OneHot for both)
‚îÇ   ‚îú‚îÄ‚îÄ run_2/                   # Separate Preprocessing
‚îÇ   ‚îú‚îÄ‚îÄ run_2.2/                 # ‚≠ê Best! (Hyperparameter + Threshold)
‚îÇ   ‚îú‚îÄ‚îÄ run_2_tuned/             # Hyperparameter tuning results
‚îÇ   ‚îú‚îÄ‚îÄ run_3/                   # SMOTE Resampling
‚îÇ   ‚îú‚îÄ‚îÄ run_4/                   # ADASYN Resampling
‚îÇ   ‚îú‚îÄ‚îÄ run_5/                   # SMOTETomek Resampling
‚îÇ   ‚îî‚îÄ‚îÄ run_6/                   # Cost-Sensitive Learning
‚îÇ       ‚îú‚îÄ‚îÄ logistic_regression.pkl
‚îÇ       ‚îú‚îÄ‚îÄ xgboost.pkl
‚îÇ       ‚îú‚îÄ‚îÄ preprocessor_lr.pkl
‚îÇ       ‚îî‚îÄ‚îÄ preprocessor_xgb.pkl
‚îÇ
‚îú‚îÄ‚îÄ plots/                       # Visualizations (‡πÅ‡∏¢‡∏Å‡∏ï‡∏≤‡∏° run)
‚îÇ   ‚îú‚îÄ‚îÄ run_1/
‚îÇ   ‚îú‚îÄ‚îÄ run_2/
‚îÇ   ‚îú‚îÄ‚îÄ run_2.2/                 # ‚≠ê Best model visualizations
‚îÇ   ‚îú‚îÄ‚îÄ run_3/
‚îÇ   ‚îú‚îÄ‚îÄ run_4/
‚îÇ   ‚îú‚îÄ‚îÄ run_5/
‚îÇ   ‚îî‚îÄ‚îÄ run_6/
‚îÇ       ‚îú‚îÄ‚îÄ confusion_matrix_lr.png
‚îÇ       ‚îú‚îÄ‚îÄ confusion_matrix_xgb.png
‚îÇ       ‚îú‚îÄ‚îÄ roc_curves.png
‚îÇ       ‚îú‚îÄ‚îÄ precision_recall_curves.png
‚îÇ       ‚îú‚îÄ‚îÄ feature_importance_lr.png
‚îÇ       ‚îú‚îÄ‚îÄ feature_importance_xgb.png
‚îÇ       ‚îú‚îÄ‚îÄ shap_summary.png
‚îÇ       ‚îú‚îÄ‚îÄ shap_bar.png
‚îÇ       ‚îú‚îÄ‚îÄ shap_waterfall_sample0.png
‚îÇ       ‚îú‚îÄ‚îÄ shap_waterfall_churn.png
‚îÇ       ‚îî‚îÄ‚îÄ shap_dependence_top.png
‚îÇ
‚îú‚îÄ‚îÄ experiments/                 # Experiment results
‚îÇ   ‚îî‚îÄ‚îÄ run_2.1_threshold_tuning/
‚îÇ       ‚îú‚îÄ‚îÄ threshold_results.csv
‚îÇ       ‚îî‚îÄ‚îÄ threshold_tuning_analysis.png
‚îÇ
‚îî‚îÄ‚îÄ Doc/
    ‚îú‚îÄ‚îÄ runs/                    # ‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡πÅ‡∏ï‡πà‡∏•‡∏∞ run
    ‚îÇ   ‚îú‚îÄ‚îÄ README.md
    ‚îÇ   ‚îú‚îÄ‚îÄ run_01_baseline.md
    ‚îÇ   ‚îú‚îÄ‚îÄ run_02_class_weights.md
    ‚îÇ   ‚îú‚îÄ‚îÄ run_02.2_threshold_tuned.md  # ‚≠ê Best model details
    ‚îÇ   ‚îú‚îÄ‚îÄ run_03_smote.md
    ‚îÇ   ‚îú‚îÄ‚îÄ run_04_adasyn.md
    ‚îÇ   ‚îú‚îÄ‚îÄ run_05_smotetomek.md
    ‚îÇ   ‚îî‚îÄ‚îÄ run_06_cost_sensitive.md
    ‚îú‚îÄ‚îÄ walkthrough.md           # ‡∏Ñ‡∏π‡πà‡∏°‡∏∑‡∏≠‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡πÇ‡∏î‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î
    ‚îú‚îÄ‚îÄ RESULTS.md               # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏•‡∏≠‡∏á‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
    ‚îî‚îÄ‚îÄ COST_SENSITIVE_GUIDE.md  # ‡∏Ñ‡∏π‡πà‡∏°‡∏∑‡∏≠ Cost-Sensitive Learning
```

## üéØ Features

### Data Preparation

- ‚úÖ Error handling ‡πÅ‡∏•‡∏∞ validation
- ‚úÖ DEBUG-level logging
- ‚úÖ Feature binning (Age, CreditScore, Tenure, Balance)
- ‚úÖ **Separate preprocessing pipelines:**
  - **Logistic Regression:** OneHot encoding (25 features)
  - **XGBoost:** Label encoding (10 features) - ‡∏î‡∏µ‡∏Å‡∏ß‡πà‡∏≤ OneHot!
- ‚úÖ Train/Val/Test split (70/15/15) ‡πÅ‡∏ö‡∏ö stratified

### Model Optimization

- ‚úÖ **Hyperparameter Tuning** (Run #2.1)
  - RandomizedSearchCV with 50 iterations
  - Custom scorer (Recall 60% + F1 40%)
  - Best params: n_estimators=50, max_depth=3, learning_rate=0.1
- ‚úÖ **Threshold Tuning** (Run #2.2)

  - Tested thresholds from 0.1 to 0.99
  - Optimal threshold: 0.54 for best balance
  - Maximizes F1 Score while maintaining Recall >= 70%

- ‚úÖ **Cost-Sensitive Learning** (Run #6)
  - Sample weighting for imbalanced data
  - Extreme Recall (91.83%) for special campaigns

### Imbalance Handling (Tested 5 Approaches)

- ‚úÖ **Class Weights** (Run #2) - Good baseline
- ‚úÖ **Hyperparameter Tuning** (Run #2.1) - High Recall
- ‚úÖ **Threshold Tuning** (Run #2.2) - ‚≠ê **Most Balanced!**
- ‚úÖ **SMOTE** (Run #3) - Overfitting
- ‚úÖ **ADASYN** (Run #4) - Overfitting
- ‚úÖ **SMOTETomek** (Run #5) - Overfitting
- ‚úÖ **Cost-Sensitive** (Run #6) - Extreme Recall

**‡∏™‡∏£‡∏∏‡∏õ:** Hyperparameter Tuning + Threshold 0.54 ‡πÉ‡∏´‡πâ‡∏ú‡∏•‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î!

### Model Training

- ‚úÖ Logistic Regression with `class_weight='balanced'`
- ‚úÖ XGBoost with optimized hyperparameters
- ‚úÖ 5-Fold Cross-Validation
- ‚úÖ Comprehensive metrics (Accuracy, Precision, Recall, F1, ROC-AUC)
- ‚úÖ **Versioned runs** - ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏ú‡∏•‡∏ó‡∏∏‡∏Å experiment

### Evaluation & Explainability

- ‚úÖ Confusion Matrix (‡πÅ‡∏¢‡∏Å LR ‡πÅ‡∏•‡∏∞ XGB)
- ‚úÖ ROC Curves (‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö LR vs XGB)
- ‚úÖ Precision-Recall Curves
- ‚úÖ Feature Importance (LR coefficients ‡πÅ‡∏•‡∏∞ XGB weights)
- ‚úÖ **SHAP Analysis:**
  - Summary Plot - ‡∏†‡∏≤‡∏û‡∏£‡∏ß‡∏° feature importance
  - Bar Plot - Mean absolute SHAP values
  - Waterfall Plots - ‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏•‡∏π‡∏Å‡∏Ñ‡πâ‡∏≤
  - Dependence Plot - ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏±‡∏°‡∏û‡∏±‡∏ô‡∏ò‡πå‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á features

## üìä ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå

‡∏î‡∏π‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏•‡∏≠‡∏á‡πÇ‡∏î‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡πÑ‡∏î‡πâ‡∏ó‡∏µ‡πà [Doc/RESULTS.md](Doc/RESULTS.md)

### üèÜ Best Model: Run #2.2 (Hyperparameter Tuned + Threshold 0.54)

**XGBoost Performance (Test Set):**

| Metric        | Score      | Status                         |
| ------------- | ---------- | ------------------------------ |
| **F1 Score**  | **0.5811** | üèÜ **‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î!**                 |
| **ROC-AUC**   | **0.8461** | ‚úÖ ‡πÄ‡∏Å‡∏¥‡∏ô‡πÄ‡∏õ‡πâ‡∏≤ 0.80               |
| **Recall**    | **0.7026** | ‚úÖ ‡πÄ‡∏Å‡∏¥‡∏ô‡πÄ‡∏õ‡πâ‡∏≤ 0.70               |
| **Precision** | **0.4954** | ‚úÖ ‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î‡πÉ‡∏ô‡∏Å‡∏•‡∏∏‡πà‡∏° Recall >= 70% |
| **Accuracy**  | **0.7933** | ‚úÖ ‡∏î‡∏µ‡∏°‡∏≤‡∏Å                       |

**Optimized Hyperparameters:**

- `n_estimators`: 50
- `max_depth`: 3
- `learning_rate`: 0.1
- `subsample`: 0.6
- `reg_lambda`: 0.1
- `reg_alpha`: 0.5
- `threshold`: 0.54

**Top 3 Features (SHAP Analysis):**

1. **Balance** (0.7238) - ‡∏¢‡∏≠‡∏î‡πÄ‡∏á‡∏¥‡∏ô‡πÉ‡∏ô‡∏ö‡∏±‡∏ç‡∏ä‡∏µ (‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î!)
2. **NumOfProducts** (0.6868) - ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô products (3-4 = Churn ‡∏™‡∏π‡∏á, 2 = ‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î)
3. **IsActiveMember** (0.3250) - ‡∏•‡∏π‡∏Å‡∏Ñ‡πâ‡∏≤ Active ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà (‡πÑ‡∏°‡πà Active = Churn ‡∏™‡∏π‡∏á‡∏°‡∏≤‡∏Å)

### üìä ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î (Top 5)

| Run  | Method                         | ROC-AUC    | Recall     | Precision  | F1         | Ranking           |
| ---- | ------------------------------ | ---------- | ---------- | ---------- | ---------- | ----------------- |
| #2.2 | **Hyperparameter + T=0.54** ‚≠ê | **0.8461** | **0.7026** | **0.4954** | **0.5811** | ü•á **Best**       |
| #2.1 | Hyperparameter Tuned           | **0.8461** | **0.7451** | 0.4740     | 0.5794     | ü•à High Recall    |
| #2   | Class Weights                  | 0.8379     | 0.6895     | 0.4862     | 0.5703     | ü•â Baseline       |
| #6   | Cost-Sensitive                 | 0.8220     | **0.9183** | 0.2838     | 0.4336     | üéØ Extreme Recall |
| #3   | SMOTE                          | 0.8170     | 0.6144     | 0.5123     | 0.5587     | 4th               |

**üí° Key Findings:**

1. **Hyperparameter Tuning** ‡πÄ‡∏û‡∏¥‡πà‡∏° Recall ‡∏à‡∏≤‡∏Å 68.95% ‚Üí 74.51% (+5.56 pp)
2. **Threshold 0.54** ‡πÉ‡∏´‡πâ F1 Score ‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î (58.11%) ‡πÅ‡∏•‡∏∞ Balance ‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î
3. **ROC-AUC = 84.61%** ‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î (‡πÄ‡∏Å‡∏¥‡∏ô‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢ 80%)
4. **Recall = 70.26%** ‡πÄ‡∏Å‡∏¥‡∏ô‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢ 70% ‡∏û‡∏≠‡∏î‡∏µ
5. **Synthetic sampling ‡∏ó‡∏∏‡∏Å‡∏ß‡∏¥‡∏ò‡∏µ‡∏™‡∏£‡πâ‡∏≤‡∏á overfitting** - ‡πÑ‡∏°‡πà‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥!

### üéØ Business Impact (Run #2.2)

- **‡∏õ‡∏£‡∏∞‡∏´‡∏¢‡∏±‡∏î‡πÑ‡∏î‡πâ:** 12.78 ‡∏•‡πâ‡∏≤‡∏ô‡∏ö‡∏≤‡∏ó/‡∏õ‡∏µ (‡∏à‡∏≤‡∏Å‡∏•‡∏π‡∏Å‡∏Ñ‡πâ‡∏≤ 2,000 ‡∏Ñ‡∏ô)
- **ROI:** 5,789% üöÄ (‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î!)
- **‡∏£‡∏±‡∏Å‡∏©‡∏≤‡∏•‡∏π‡∏Å‡∏Ñ‡πâ‡∏≤‡πÑ‡∏ß‡πâ‡πÑ‡∏î‡πâ:** 65 ‡∏Ñ‡∏ô (‡∏°‡∏π‡∏•‡∏Ñ‡πà‡∏≤ 6.5 ‡∏•‡πâ‡∏≤‡∏ô‡∏ö‡∏≤‡∏ó)
- **Churn Rate ‡∏•‡∏î‡∏•‡∏á:** ‡∏à‡∏≤‡∏Å 15.3% ‚Üí 12.1%
- **‡∏ï‡πâ‡∏ô‡∏ó‡∏∏‡∏ô‡∏ï‡πà‡∏≥‡∏™‡∏∏‡∏î:** 217,000 ‡∏ö‡∏≤‡∏ó (‡∏ï‡∏¥‡∏î‡∏ï‡πà‡∏≠ 434 ‡∏Ñ‡∏ô)

### üéØ Model Selection Guide

**‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£:**

- **‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ (‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ Balance)** ‚Üí Run #2.2 ‚≠ê **‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥!**
  - F1 Score ‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î, Balance ‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î, ROI ‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î
- **‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ Recall ‡∏™‡∏π‡∏á** ‚Üí Run #2.1 üöÄ
  - Recall = 74.51%, ROC-AUC = 84.61%
- **‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ Simplicity** ‚Üí Run #2
  - ‡πÉ‡∏ä‡πâ default hyperparameters, ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏î‡∏µ
- **Campaign ‡∏û‡∏¥‡πÄ‡∏®‡∏© (‡∏¢‡∏≠‡∏°‡∏£‡∏±‡∏ö False Positive ‡∏™‡∏π‡∏á)** ‚Üí Run #6 üéØ
  - Recall = 91.83% (‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î!)

## üìñ ‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£

- **[Walkthrough](Doc/walkthrough.md)** - ‡∏Ñ‡∏π‡πà‡∏°‡∏∑‡∏≠‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡πÇ‡∏î‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î
- **[Results](Doc/RESULTS.md)** - ‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏•‡∏≠‡∏á‡πÅ‡∏•‡∏∞ metrics ‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏£‡∏≠‡∏ö
- **[Run #2.2 Details](Doc/runs/run_02.2_threshold_tuned.md)** - ‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î Best Model
- **[Cost-Sensitive Guide](Doc/COST_SENSITIVE_GUIDE.md)** - ‡∏Ñ‡∏π‡πà‡∏°‡∏∑‡∏≠ Cost-Sensitive Learning

## üõ†Ô∏è Technologies

- Python 3.12
- **Machine Learning:**
  - scikit-learn - Logistic Regression, preprocessing, GridSearchCV
  - XGBoost - Gradient boosting with hyperparameter tuning
  - imbalanced-learn - SMOTE, ADASYN, SMOTETomek
- **Explainability:**
  - SHAP - Model interpretation
- **Data Processing:**
  - pandas, numpy
- **Visualization:**
  - matplotlib, seaborn

## üìù License

This project is for educational purposes.

## üë§ Author

Created as part of a Customer Churn Prediction project.

**Last Updated:** 2026-01-16

**Total Experiments:** 8 Runs (6 main + Hyperparameter Tuning + Threshold Tuning)

**Best Model:** Run #2.2 - Hyperparameter Tuned + Threshold 0.54 ‚≠ê
