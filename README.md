# Customer Churn Prediction

à¸£à¸°à¸šà¸šà¸—à¸³à¸™à¸²à¸¢à¸à¸²à¸£à¸«à¸¢à¸¸à¸”à¹ƒà¸Šà¹‰à¸šà¸£à¸´à¸à¸²à¸£à¸‚à¸­à¸‡à¸¥à¸¹à¸à¸„à¹‰à¸²à¸˜à¸™à¸²à¸„à¸²à¸£ (Customer Churn) à¹‚à¸”à¸¢à¹ƒà¸Šà¹‰ Machine Learning

## ğŸ“Š à¸ à¸²à¸à¸£à¸§à¸¡à¹‚à¸›à¸£à¹€à¸ˆà¸à¸•à¹Œ

à¹‚à¸›à¸£à¹€à¸ˆà¸à¸•à¹Œà¸™à¸µà¹‰à¹ƒà¸Šà¹‰à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸¥à¸¹à¸à¸„à¹‰à¸²à¸˜à¸™à¸²à¸„à¸²à¸£à¹€à¸à¸·à¹ˆà¸­à¸—à¸³à¸™à¸²à¸¢à¸§à¹ˆà¸²à¸¥à¸¹à¸à¸„à¹‰à¸²à¸„à¸™à¹„à¸«à¸™à¸¡à¸µà¹à¸™à¸§à¹‚à¸™à¹‰à¸¡à¸ˆà¸°à¸«à¸¢à¸¸à¸”à¹ƒà¸Šà¹‰à¸šà¸£à¸´à¸à¸²à¸£ (Churn) à¹‚à¸”à¸¢à¹ƒà¸Šà¹‰:

- **Logistic Regression** (Baseline model)
- **XGBoost** (High-performance model)
- **SHAP** à¸ªà¸³à¸«à¸£à¸±à¸šà¸­à¸˜à¸´à¸šà¸²à¸¢à¸œà¸¥à¸à¸²à¸£à¸—à¸³à¸™à¸²à¸¢

## ğŸš€ Quick Start

### 1. à¸•à¸´à¸”à¸•à¸±à¹‰à¸‡ Dependencies

```bash
pip install -r requirements.txt
```

### 2. à¸£à¸±à¸™ Best Model (Run #2)

```bash
# Train models with separate preprocessing (à¹à¸™à¸°à¸™à¸³!)
python train_models.py --version 2

# à¸ªà¸£à¹‰à¸²à¸‡ visualizations
python evaluate_models.py --version 2

# à¸§à¸´à¹€à¸„à¸£à¸²à¸°à¸«à¹Œà¸”à¹‰à¸§à¸¢ SHAP
python shap_analysis.py --version 2
```

### 3. à¸—à¸”à¸¥à¸­à¸‡ Imbalance Handling à¸­à¸·à¹ˆà¸™à¹† (Optional)

```bash
# Run #3: SMOTE
python train_models.py --version 3 --imbalance-method smote

# Run #4: ADASYN
python train_models.py --version 4 --imbalance-method adasyn

# Run #5: SMOTETomek
python train_models.py --version 5 --imbalance-method smotetomek
```

**à¸«à¸¡à¸²à¸¢à¹€à¸«à¸•à¸¸:** Run #2 (Class Weights) à¹ƒà¸«à¹‰à¸œà¸¥à¸”à¸µà¸—à¸µà¹ˆà¸ªà¸¸à¸” - à¹„à¸¡à¹ˆà¹à¸™à¸°à¸™à¸³à¹ƒà¸«à¹‰à¹ƒà¸Šà¹‰ synthetic sampling!

## ğŸ“ à¹‚à¸„à¸£à¸‡à¸ªà¸£à¹‰à¸²à¸‡à¹‚à¸›à¸£à¹€à¸ˆà¸à¸•à¹Œ

```
Customer Churn Prediction/
â”œâ”€â”€ README.md                    # à¹„à¸Ÿà¸¥à¹Œà¸™à¸µà¹‰
â”œâ”€â”€ requirements.txt             # Python dependencies
â”œâ”€â”€ config.py                    # Configuration à¹à¸¥à¸° hyperparameters
â”œâ”€â”€ logger_config.py             # Logging setup
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ Churn_Modelling.csv     # Dataset (10,000 à¸¥à¸¹à¸à¸„à¹‰à¸²)
â”‚
â”œâ”€â”€ feature_binning.py           # Custom transformers à¸ªà¸³à¸«à¸£à¸±à¸š binning
â”œâ”€â”€ imbalance_handlers.py        # SMOTE, ADASYN, SMOTETomek handlers
â”œâ”€â”€ data_prep.py                 # Data preparation pipeline
â”œâ”€â”€ train_models.py              # Model training script (à¸£à¸­à¸‡à¸£à¸±à¸š versioning)
â”œâ”€â”€ evaluate_models.py           # Evaluation & visualization
â”œâ”€â”€ shap_analysis.py             # SHAP explainability
â”œâ”€â”€ test_pipeline.py             # Pipeline testing
â”‚
â”œâ”€â”€ models/                      # Trained models (à¹à¸¢à¸à¸•à¸²à¸¡ run)
â”‚   â”œâ”€â”€ run_1/                   # Baseline (OneHot for both)
â”‚   â”œâ”€â”€ run_2/                   # Separate Preprocessing â­ Best!
â”‚   â”œâ”€â”€ run_3/                   # SMOTE Resampling
â”‚   â”œâ”€â”€ run_4/                   # ADASYN Resampling
â”‚   â””â”€â”€ run_5/                   # SMOTETomek Resampling
â”‚       â”œâ”€â”€ logistic_regression.pkl
â”‚       â”œâ”€â”€ xgboost.pkl
â”‚       â”œâ”€â”€ preprocessor_lr.pkl
â”‚       â””â”€â”€ preprocessor_xgb.pkl
â”‚
â”œâ”€â”€ plots/                       # Visualizations (à¹à¸¢à¸à¸•à¸²à¸¡ run)
â”‚   â”œâ”€â”€ run_1/
â”‚   â”œâ”€â”€ run_2/                   # â­ Best model visualizations
â”‚   â”œâ”€â”€ run_3/
â”‚   â”œâ”€â”€ run_4/
â”‚   â””â”€â”€ run_5/
â”‚       â”œâ”€â”€ confusion_matrix_lr.png
â”‚       â”œâ”€â”€ confusion_matrix_xgb.png
â”‚       â”œâ”€â”€ roc_curves.png
â”‚       â”œâ”€â”€ precision_recall_curves.png
â”‚       â”œâ”€â”€ feature_importance_lr.png
â”‚       â”œâ”€â”€ feature_importance_xgb.png
â”‚       â”œâ”€â”€ shap_summary.png
â”‚       â”œâ”€â”€ shap_bar.png
â”‚       â”œâ”€â”€ shap_waterfall_sample0.png
â”‚       â”œâ”€â”€ shap_waterfall_churn.png
â”‚       â””â”€â”€ shap_dependence_top.png
â”‚
â””â”€â”€ Doc/
    â”œâ”€â”€ walkthrough.md           # à¸„à¸¹à¹ˆà¸¡à¸·à¸­à¸à¸²à¸£à¹ƒà¸Šà¹‰à¸‡à¸²à¸™à¹‚à¸”à¸¢à¸¥à¸°à¹€à¸­à¸µà¸¢à¸”
    â””â”€â”€ RESULTS.md               # à¸šà¸±à¸™à¸—à¸¶à¸à¸œà¸¥à¸à¸²à¸£à¸—à¸”à¸¥à¸­à¸‡à¸—à¸±à¹‰à¸‡ 5 runs
```

## ğŸ¯ Features

### Data Preparation

- âœ… Error handling à¹à¸¥à¸° validation
- âœ… DEBUG-level logging
- âœ… Feature binning (Age, CreditScore, Tenure, Balance)
- âœ… **Separate preprocessing pipelines:**
  - **Logistic Regression:** OneHot encoding (25 features)
  - **XGBoost:** Label encoding (10 features) - à¸”à¸µà¸à¸§à¹ˆà¸² OneHot!
- âœ… Train/Val/Test split (70/15/15) à¹à¸šà¸š stratified

### Imbalance Handling (Tested 4 Approaches)

- âœ… **Class Weights** (Run #2) - â­ **Best approach!**
  - Logistic Regression: `class_weight='balanced'`
  - XGBoost: `scale_pos_weight=3.9088`
- âœ… **SMOTE** (Run #3) - Synthetic over-sampling
- âœ… **ADASYN** (Run #4) - Adaptive synthetic sampling
- âœ… **SMOTETomek** (Run #5) - Hybrid over/under-sampling

**à¸ªà¸£à¸¸à¸›:** Class Weights à¹ƒà¸«à¹‰à¸œà¸¥à¸”à¸µà¸—à¸µà¹ˆà¸ªà¸¸à¸” - Synthetic sampling à¸—à¸¸à¸à¸§à¸´à¸˜à¸µà¸ªà¸£à¹‰à¸²à¸‡ overfitting!

### Model Training

- âœ… Logistic Regression with `class_weight='balanced'`
- âœ… XGBoost with `scale_pos_weight` à¹à¸¥à¸° Label Encoding
- âœ… 5-Fold Cross-Validation
- âœ… Comprehensive metrics (Accuracy, Precision, Recall, F1, ROC-AUC)
- âœ… **Versioned runs** - à¸šà¸±à¸™à¸—à¸¶à¸à¸œà¸¥à¸—à¸¸à¸ experiment

### Evaluation & Explainability

- âœ… Confusion Matrix (à¹à¸¢à¸ LR à¹à¸¥à¸° XGB)
- âœ… ROC Curves (à¹€à¸›à¸£à¸µà¸¢à¸šà¹€à¸—à¸µà¸¢à¸š LR vs XGB)
- âœ… Precision-Recall Curves
- âœ… Feature Importance (LR coefficients à¹à¸¥à¸° XGB weights)
- âœ… **SHAP Analysis:**
  - Summary Plot - à¸ à¸²à¸à¸£à¸§à¸¡ feature importance
  - Bar Plot - Mean absolute SHAP values
  - Waterfall Plots - à¸­à¸˜à¸´à¸šà¸²à¸¢à¸à¸²à¸£à¸—à¸³à¸™à¸²à¸¢à¹à¸•à¹ˆà¸¥à¸°à¸¥à¸¹à¸à¸„à¹‰à¸²
  - Dependence Plot - à¸„à¸§à¸²à¸¡à¸ªà¸±à¸¡à¸à¸±à¸™à¸˜à¹Œà¸£à¸°à¸«à¸§à¹ˆà¸²à¸‡ features

## ğŸ“Š à¸œà¸¥à¸¥à¸±à¸à¸˜à¹Œ

à¸”à¸¹à¸œà¸¥à¸à¸²à¸£à¸—à¸”à¸¥à¸­à¸‡à¹‚à¸”à¸¢à¸¥à¸°à¹€à¸­à¸µà¸¢à¸”à¹„à¸”à¹‰à¸—à¸µà¹ˆ [Doc/RESULTS.md](Doc/RESULTS.md)

### ğŸ† Best Model: Run #2 (Separate Preprocessing + Class Weights)

**XGBoost Performance (Test Set):**

| Metric        | Score      | Status           |
| ------------- | ---------- | ---------------- |
| **ROC-AUC**   | **0.8379** | âœ… à¹€à¸à¸´à¸™à¹€à¸›à¹‰à¸² 0.80 |
| **Recall**    | **0.6895** | âœ… à¹ƒà¸à¸¥à¹‰à¹€à¸›à¹‰à¸² 0.70 |
| **Precision** | **0.4862** | âš ï¸ à¸•à¹ˆà¸³à¸à¸§à¹ˆà¸²à¹€à¸›à¹‰à¸²   |
| **F1 Score**  | **0.5703** | âœ… à¹ƒà¸à¸¥à¹‰à¹€à¸›à¹‰à¸² 0.65 |
| **Accuracy**  | **0.7880** | âœ… à¸”à¸µ            |

**Top 3 Features (SHAP Analysis):**

1. **Balance** - à¸¢à¸­à¸”à¹€à¸‡à¸´à¸™à¹ƒà¸™à¸šà¸±à¸à¸Šà¸µ (à¸ªà¸³à¸„à¸±à¸à¸—à¸µà¹ˆà¸ªà¸¸à¸”!)
2. **NumOfProducts** - à¸ˆà¸³à¸™à¸§à¸™ products (3-4 = Churn à¸ªà¸¹à¸‡, 2 = à¸”à¸µà¸—à¸µà¹ˆà¸ªà¸¸à¸”)
3. **IsActiveMember** - à¸¥à¸¹à¸à¸„à¹‰à¸² Active à¸«à¸£à¸·à¸­à¹„à¸¡à¹ˆ (à¹„à¸¡à¹ˆ Active = Churn à¸ªà¸¹à¸‡à¸¡à¸²à¸)

### ğŸ“Š à¹€à¸›à¸£à¸µà¸¢à¸šà¹€à¸—à¸µà¸¢à¸šà¸—à¸±à¹‰à¸‡ 5 Runs

| Run | Method                 | ROC-AUC    | Recall     | Precision  | Ranking     |
| --- | ---------------------- | ---------- | ---------- | ---------- | ----------- |
| #2  | **Class Weights** â­   | **0.8379** | **0.6895** | 0.4862     | ğŸ¥‡ **Best** |
| #3  | SMOTE                  | 0.8170     | 0.6144     | 0.5123     | ğŸ¥ˆ 2nd      |
| #5  | SMOTETomek             | 0.8121     | 0.6046     | **0.5153** | ğŸ¥‰ 3rd      |
| #4  | ADASYN                 | 0.8106     | 0.6013     | 0.5041     | 4th         |
| #1  | Baseline (OneHot both) | 0.7279     | 0.6144     | 0.3501     | 5th         |

**ğŸ’¡ Key Findings:**

1. **Separate Preprocessing** à¸—à¸³à¹ƒà¸«à¹‰ XGBoost à¸”à¸µà¸‚à¸¶à¹‰à¸™ **15%** à¹ƒà¸™ ROC-AUC (Run #1 â†’ #2)
2. **Class Weights à¸”à¸µà¸—à¸µà¹ˆà¸ªà¸¸à¸”** - Synthetic sampling à¸—à¸¸à¸à¸§à¸´à¸˜à¸µà¸ªà¸£à¹‰à¸²à¸‡ overfitting
3. **Label Encoding à¹€à¸«à¸¡à¸²à¸°à¸à¸±à¸š XGBoost** à¸¡à¸²à¸à¸à¸§à¹ˆà¸² OneHot Encoding
4. **Recall à¹€à¸›à¹‰à¸²à¸«à¸¡à¸²à¸¢à¸ªà¸³à¸„à¸±à¸** - Run #2 à¹ƒà¸«à¹‰ Recall à¸ªà¸¹à¸‡à¸ªà¸¸à¸” (68.95%)

### ğŸ¯ Business Impact (Run #2)

- **à¸›à¸£à¸°à¸«à¸¢à¸±à¸”à¹„à¸”à¹‰:** 12.5 à¸¥à¹‰à¸²à¸™à¸šà¸²à¸—/à¸›à¸µ (à¸ˆà¸²à¸à¸¥à¸¹à¸à¸„à¹‰à¸² 2,000 à¸„à¸™)
- **ROI:** 4,849% ğŸš€
- **à¸£à¸±à¸à¸©à¸²à¸¥à¸¹à¸à¸„à¹‰à¸²à¹„à¸§à¹‰à¹„à¸”à¹‰:** 64 à¸„à¸™ (à¸¡à¸¹à¸¥à¸„à¹ˆà¸² 6.4 à¸¥à¹‰à¸²à¸™à¸šà¸²à¸—)
- **Churn Rate à¸¥à¸”à¸¥à¸‡:** à¸ˆà¸²à¸ 15.3% â†’ 12.1%

## ğŸ“– à¹€à¸­à¸à¸ªà¸²à¸£

- **[Walkthrough](Doc/walkthrough.md)** - à¸„à¸¹à¹ˆà¸¡à¸·à¸­à¸à¸²à¸£à¹ƒà¸Šà¹‰à¸‡à¸²à¸™à¹‚à¸”à¸¢à¸¥à¸°à¹€à¸­à¸µà¸¢à¸”
- **[Results](Doc/RESULTS.md)** - à¸œà¸¥à¸à¸²à¸£à¸—à¸”à¸¥à¸­à¸‡à¹à¸¥à¸° metrics à¹à¸•à¹ˆà¸¥à¸°à¸£à¸­à¸š

## ğŸ› ï¸ Technologies

- Python 3.x
- **Machine Learning:**
  - scikit-learn - Logistic Regression, preprocessing
  - XGBoost - Gradient boosting
  - imbalanced-learn - SMOTE, ADASYN, SMOTETomek
- **Explainability:**
  - SHAP - Model interpretation
- **Data Processing:**
  - pandas, numpy
- **Visualization:**
  - matplotlib, seaborn

## ğŸ“ License

This project is for educational purposes.

## ğŸ‘¤ Author

Created as part of a Customer Churn Prediction project.
