# Customer Churn Prediction

A Machine Learning system to predict customer churn for banking services

## ğŸ“Š Project Overview

This project uses bank customer data to predict which customers are likely to churn (stop using services) using:

- **Logistic Regression** (Baseline model)
- **XGBoost** (High-performance model)
- **Hyperparameter Tuning** (Optimized model)
- **Threshold Tuning** (Balanced predictions)
- **SHAP** for model interpretability

## ğŸš€ Quick Start

### 1. Install Dependencies

```bash
pip install -r requirements.txt
```

### 2. Run Best Model (Run #2.2 - Hyperparameter Tuned + Threshold 0.54)

```bash
# Train models with optimized hyperparameters
python train_models.py

# Generate visualizations
python evaluate_models.py

# Analyze with SHAP
python shap_analysis.py
```

### 3. Try Other Approaches (Optional)

```bash
# Hyperparameter Tuning
python hyperparameter_tuning.py

# Threshold Tuning
python threshold_tuning.py

# Cost-Sensitive Learning
python train_models.py --cost-sensitive
```

**Note:** Run #2.2 (Hyperparameter Tuned + Threshold 0.54) gives the best results!

## ğŸ“ Project Structure

```
Customer Churn Prediction/
â”œâ”€â”€ README.md                    # This file
â”œâ”€â”€ requirements.txt             # Python dependencies
â”œâ”€â”€ config.py                    # Configuration and hyperparameters
â”œâ”€â”€ logger_config.py             # Logging setup
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ Churn_Modelling.csv     # Dataset (10,000 customers)
â”‚
â”œâ”€â”€ feature_binning.py           # Custom transformers for binning
â”œâ”€â”€ imbalance_handlers.py        # SMOTE, ADASYN, SMOTETomek handlers
â”œâ”€â”€ cost_sensitive.py            # Cost-sensitive learning utilities
â”œâ”€â”€ data_prep.py                 # Data preparation pipeline
â”œâ”€â”€ train_models.py              # Model training script
â”œâ”€â”€ evaluate_models.py           # Evaluation & visualization
â”œâ”€â”€ shap_analysis.py             # SHAP explainability
â”œâ”€â”€ hyperparameter_tuning.py     # Hyperparameter optimization
â”œâ”€â”€ threshold_tuning.py          # Threshold optimization
â”‚
â”œâ”€â”€ models/                      # Trained models (separated by run)
â”‚   â”œâ”€â”€ run_1/                   # Baseline (OneHot for both)
â”‚   â”œâ”€â”€ run_2/                   # Separate Preprocessing
â”‚   â”œâ”€â”€ run_2.2/                 # â­ Best! (Hyperparameter + Threshold)
â”‚   â”œâ”€â”€ run_2_tuned/             # Hyperparameter tuning results
â”‚   â”œâ”€â”€ run_3/                   # SMOTE Resampling
â”‚   â”œâ”€â”€ run_4/                   # ADASYN Resampling
â”‚   â”œâ”€â”€ run_5/                   # SMOTETomek Resampling
â”‚   â””â”€â”€ run_6/                   # Cost-Sensitive Learning
â”‚       â”œâ”€â”€ logistic_regression.pkl
â”‚       â”œâ”€â”€ xgboost.pkl
â”‚       â”œâ”€â”€ preprocessor_lr.pkl
â”‚       â””â”€â”€ preprocessor_xgb.pkl
â”‚
â”œâ”€â”€ plots/                       # Visualizations (separated by run)
â”‚   â”œâ”€â”€ run_1/
â”‚   â”œâ”€â”€ run_2/
â”‚   â”œâ”€â”€ run_2.2/                 # â­ Best model visualizations
â”‚   â”œâ”€â”€ run_3/
â”‚   â”œâ”€â”€ run_4/
â”‚   â”œâ”€â”€ run_5/
â”‚   â””â”€â”€ run_6/
â”‚       â”œâ”€â”€ confusion_matrix_lr.png
â”‚       â”œâ”€â”€ confusion_matrix_xgb.png
â”‚       â”œâ”€â”€ roc_curves.png
â”‚       â”œâ”€â”€ precision_recall_curves.png
â”‚       â”œâ”€â”€ feature_importance_lr.png
â”‚       â”œâ”€â”€ feature_importance_xgb.png
â”‚       â”œâ”€â”€ shap_summary.png
â”‚       â”œâ”€â”€ shap_bar.png
â”‚       â”œâ”€â”€ shap_waterfall_sample0.png
â”‚       â”œâ”€â”€ shap_waterfall_churn.png
â”‚       â””â”€â”€ shap_dependence_top.png
â”‚
â”œâ”€â”€ experiments/                 # Experiment results
â”‚   â””â”€â”€ run_2.1_threshold_tuning/
â”‚       â”œâ”€â”€ threshold_results.csv
â”‚       â””â”€â”€ threshold_tuning_analysis.png
â”‚
â””â”€â”€ Doc/
    â”œâ”€â”€ runs/                    # Detailed run documentation
    â”‚   â”œâ”€â”€ README.md
    â”‚   â”œâ”€â”€ run_01_baseline.md
    â”‚   â”œâ”€â”€ run_02_class_weights.md
    â”‚   â”œâ”€â”€ run_02.2_threshold_tuned.md  # â­ Best model details
    â”‚   â”œâ”€â”€ run_03_smote.md
    â”‚   â”œâ”€â”€ run_04_adasyn.md
    â”‚   â”œâ”€â”€ run_05_smotetomek.md
    â”‚   â””â”€â”€ run_06_cost_sensitive.md
    â”œâ”€â”€ walkthrough.md           # Detailed user guide
    â”œâ”€â”€ RESULTS.md               # All experiment results
    â””â”€â”€ COST_SENSITIVE_GUIDE.md  # Cost-Sensitive Learning guide
```

## ğŸ¯ Features

### Data Preparation

- âœ… Error handling and validation
- âœ… DEBUG-level logging
- âœ… Feature binning (Age, CreditScore, Tenure, Balance)
- âœ… **Separate preprocessing pipelines:**
  - **Logistic Regression:** OneHot encoding (25 features)
  - **XGBoost:** Label encoding (10 features) - Better than OneHot!
- âœ… Train/Val/Test split (70/15/15) with stratification

### Model Optimization

- âœ… **Hyperparameter Tuning** (Run #2.1)
  - RandomizedSearchCV with 50 iterations
  - Custom scorer (Recall 60% + F1 40%)
  - Best params: n_estimators=50, max_depth=3, learning_rate=0.1
- âœ… **Threshold Tuning** (Run #2.2)

  - Tested thresholds from 0.1 to 0.99
  - Optimal threshold: 0.54 for best balance
  - Maximizes F1 Score while maintaining Recall >= 70%

- âœ… **Cost-Sensitive Learning** (Run #6)
  - Sample weighting for imbalanced data
  - Extreme Recall (91.83%) for special campaigns

### Imbalance Handling (Tested 7 Approaches)

- âœ… **Class Weights** (Run #2) - Good baseline
- âœ… **Hyperparameter Tuning** (Run #2.1) - High Recall
- âœ… **Threshold Tuning** (Run #2.2) - â­ **Most Balanced!**
- âœ… **SMOTE** (Run #3) - Overfitting
- âœ… **ADASYN** (Run #4) - Overfitting
- âœ… **SMOTETomek** (Run #5) - Overfitting
- âœ… **Cost-Sensitive** (Run #6) - Extreme Recall

**Conclusion:** Hyperparameter Tuning + Threshold 0.54 gives the best results!

### Model Training

- âœ… Logistic Regression with `class_weight='balanced'`
- âœ… XGBoost with optimized hyperparameters
- âœ… 5-Fold Cross-Validation
- âœ… Comprehensive metrics (Accuracy, Precision, Recall, F1, ROC-AUC)
- âœ… **Versioned runs** - Track all experiments

### Evaluation & Explainability

- âœ… Confusion Matrix (separate for LR and XGB)
- âœ… ROC Curves (compare LR vs XGB)
- âœ… Precision-Recall Curves
- âœ… Feature Importance (LR coefficients and XGB weights)
- âœ… **SHAP Analysis:**
  - Summary Plot - Overall feature importance
  - Bar Plot - Mean absolute SHAP values
  - Waterfall Plots - Explain individual predictions
  - Dependence Plot - Feature relationships

## ğŸ“Š Results

See detailed experiment results at [Doc/RESULTS.md](Doc/RESULTS.md)

### ğŸ† Best Model: Run #2.2 (Hyperparameter Tuned + Threshold 0.54)

**XGBoost Performance (Test Set):**

| Metric        | Score      | Status                            |
| ------------- | ---------- | --------------------------------- |
| **F1 Score**  | **0.5811** | ğŸ† **Highest!**                   |
| **ROC-AUC**   | **0.8461** | âœ… Exceeds target 0.80            |
| **Recall**    | **0.7026** | âœ… Exceeds target 0.70            |
| **Precision** | **0.4954** | âœ… Highest in Recall >= 70% group |
| **Accuracy**  | **0.7933** | âœ… Excellent                      |

**Optimized Hyperparameters:**

- `n_estimators`: 50
- `max_depth`: 3
- `learning_rate`: 0.1
- `subsample`: 0.6
- `reg_lambda`: 0.1
- `reg_alpha`: 0.5
- `threshold`: 0.54

**Top 3 Features (SHAP Analysis):**

1. **Balance** (0.7238) - Account balance (Most important!)
2. **NumOfProducts** (0.6868) - Number of products (3-4 = High churn, 2 = Best)
3. **IsActiveMember** (0.3250) - Active customer status (Inactive = High churn)

### ğŸ“Š Comparison (Top 5 Runs)

| Run  | Method                         | ROC-AUC    | Recall     | Precision  | F1         | Ranking           |
| ---- | ------------------------------ | ---------- | ---------- | ---------- | ---------- | ----------------- |
| #2.2 | **Hyperparameter + T=0.54** â­ | **0.8461** | **0.7026** | **0.4954** | **0.5811** | ğŸ¥‡ **Best**       |
| #2.1 | Hyperparameter Tuned           | **0.8461** | **0.7451** | 0.4740     | 0.5794     | ğŸ¥ˆ High Recall    |
| #2   | Class Weights                  | 0.8379     | 0.6895     | 0.4862     | 0.5703     | ğŸ¥‰ Baseline       |
| #6   | Cost-Sensitive                 | 0.8220     | **0.9183** | 0.2838     | 0.4336     | ğŸ¯ Extreme Recall |
| #3   | SMOTE                          | 0.8170     | 0.6144     | 0.5123     | 0.5587     | 4th               |

**ğŸ’¡ Key Findings:**

1. **Hyperparameter Tuning** increased Recall from 68.95% â†’ 74.51% (+5.56 pp)
2. **Threshold 0.54** achieves highest F1 Score (58.11%) with best balance
3. **ROC-AUC = 84.61%** - Highest (exceeds 80% target)
4. **Recall = 70.26%** - Exceeds 70% target perfectly
5. **Synthetic sampling causes overfitting** - Not recommended!

### ğŸ¯ Business Impact (Run #2.2)

- **Cost Savings:** 12.78M THB/year (from 2,000 customers)
- **ROI:** 5,789% ğŸš€ (Highest!)
- **Customers Retained:** 65 customers (worth 6.5M THB)
- **Churn Rate Reduction:** From 15.3% â†’ 12.1%
- **Lowest Cost:** 217,000 THB (contact 434 customers)

### ğŸ¯ Model Selection Guide

**For Banks:**

- **General Banks (Need Balance)** â†’ Run #2.2 â­ **Recommended!**
  - Highest F1 Score, Best balance, Highest ROI
- **Banks Needing High Recall** â†’ Run #2.1 ğŸš€
  - Recall = 74.51%, ROC-AUC = 84.61%
- **Banks Needing Simplicity** â†’ Run #2
  - Uses default hyperparameters, good results
- **Special Campaigns (Accept High False Positives)** â†’ Run #6 ğŸ¯
  - Recall = 91.83% (Highest!)

## ğŸ“– Documentation

- **[Walkthrough](Doc/walkthrough.md)** - Detailed user guide
- **[Results](Doc/RESULTS.md)** - All experiment results and metrics
- **[Run #2.2 Details](Doc/runs/run_02.2_threshold_tuned.md)** - Best model details
- **[Cost-Sensitive Guide](Doc/COST_SENSITIVE_GUIDE.md)** - Cost-Sensitive Learning guide

## ğŸ› ï¸ Technologies

- Python 3.12
- **Machine Learning:**
  - scikit-learn - Logistic Regression, preprocessing, GridSearchCV
  - XGBoost - Gradient boosting with hyperparameter tuning
  - imbalanced-learn - SMOTE, ADASYN, SMOTETomek
- **Explainability:**
  - SHAP - Model interpretation
- **Data Processing:**
  - pandas, numpy
- **Visualization:**
  - matplotlib, seaborn

## ğŸ“ License

This project is for educational purposes.

## ğŸ‘¤ Author

Created as part of a Customer Churn Prediction project.

**Last Updated:** 2026-01-16

**Total Experiments:** 8 Runs (6 main + Hyperparameter Tuning + Threshold Tuning)

**Best Model:** Run #2.2 - Hyperparameter Tuned + Threshold 0.54 â­
